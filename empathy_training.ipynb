{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empathetic Dialogues dataset fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from accelerate) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from accelerate) (2.6.0+cu126)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from accelerate) (0.29.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from accelerate) (0.5.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.12.0)\n",
      "Requirement already satisfied: requests in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from torch>=2.0.0->accelerate) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from tensorboard) (1.70.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from tensorboard) (2.2.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from tensorboard) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from tensorboard) (5.29.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from tensorboard) (75.8.0)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers\n",
    "!pip install datasets\n",
    "!pip install -U accelerate\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Diego\\OneDrive\\Desktop\\empathAI\\caldito\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # to get the predictions\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conv_id', 'situation', 'emotion', 'conversations'],\n",
      "        num_rows: 19533\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['conv_id', 'situation', 'emotion', 'conversations'],\n",
      "        num_rows: 2770\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['conv_id', 'situation', 'emotion', 'conversations'],\n",
      "        num_rows: 2547\n",
      "    })\n",
      "})\n",
      "('I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.', 'Was this a friend you were in love with, or just a best friend?')\n",
      "('Today,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!', 'Are you fine now?')\n"
     ]
    }
   ],
   "source": [
    "empathetic_dialogues_ds = load_dataset(\"Estwld/empathetic_dialogues_llm\")\n",
    "print(empathetic_dialogues_ds)\n",
    "\n",
    "def extract_conversation_pairs(example):\n",
    "    pairs = []\n",
    "    for i in range(len(example['conversations']) - 1):\n",
    "        if example['conversations'][i]['role'] == 'user' and example['conversations'][i + 1]['role'] == 'assistant':\n",
    "            user_input = example['conversations'][i]['content']\n",
    "            assistant_reponse = example['conversations'][i + 1]['content']\n",
    "            pairs.append((user_input, assistant_reponse))\n",
    "    return pairs\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    all_pairs = []\n",
    "    for example in dataset:\n",
    "        all_pairs.extend(extract_conversation_pairs(example))\n",
    "    return all_pairs\n",
    "\n",
    "train_pairs = preprocess_dataset(empathetic_dialogues_ds['train'])\n",
    "eval_pairs = preprocess_dataset(empathetic_dialogues_ds['valid'])\n",
    "print(train_pairs[0])\n",
    "print(eval_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer and model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"microsoft/DialoGPT-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Requirement already satisfied: torch in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (2.6.0+cu126)\n",
      "Requirement already satisfied: torchvision in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (0.21.0+cu126)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (2.6.0+cu126)\n",
      "Requirement already satisfied: filelock in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from torchvision) (2.2.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# tokenize the dataset\n",
    "def tokenize_pairs(pairs):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for user_input, assistant_response in pairs:\n",
    "        encoded = tokenizer(\n",
    "            user_input,\n",
    "            assistant_response,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        inputs.append(encoded['input_ids'])\n",
    "        labels.append(encoded['attention_mask'])\n",
    "    return {'input_ids': torch.cat(inputs), 'attention_mask': torch.cat(labels)}\n",
    "\n",
    "train_data = tokenize_pairs(train_pairs)\n",
    "eval_data = tokenize_pairs(eval_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 4090\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# ensure the correct GPU is being used (if multiple are available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = torch.tensor([1.0, 2.0, 3.0]).to(device)\n",
    "print(x.device) # should print 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# define the custom Dataset class for processing dialog data\n",
    "class DialogDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        \"\"\"\n",
    "        initializes the DialogDataset with the encoded text data.\n",
    "\n",
    "        :param encodings: a dictionary containing the encoded input data.\n",
    "                           it should contain keys like 'input_ids' and 'attention_mask'\n",
    "                           after tokenizing the raw text.\n",
    "        \"\"\"\n",
    "        # store the encodings (tokenized inputs) passed to the class\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" \n",
    "        returns the length of the dataset.\n",
    "\n",
    "        the length is determined by the number of input sequences in the 'input_ids'\n",
    "        of the encodings dictionary. all items (input sequences) should have the same length.\n",
    "\n",
    "        :return: the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        # return the number of samples, which is the same as the length of the 'input_ids' list\n",
    "        return len(self.encodings['input_ids'])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        retrieves a sample from the dataset at the specified index.\n",
    "\n",
    "        the method returns the tokenized input for a given index (sample)\n",
    "        in the dataset. the sample consists of input IDs and other related features\n",
    "        (e.g., attention mask_mask) wrapped as a tensor.\n",
    "\n",
    "        :param idx: the index of the sample to retrieve.\n",
    "        :return: a dictionary where each key is a feature (e.g., 'input_ids', 'attention_mask')\n",
    "        \"\"\"\n",
    "        # create and return a dictionary with each encoding key and its corresponding value\n",
    "        # at the specified index. the values are converted into PyTorch tensors.\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = item['input_ids'].clone() # labels should be the same as input_ids\n",
    "        return item\n",
    "        \n",
    "train_dataset = DialogDataset(train_data)\n",
    "eval_dataset = DialogDataset(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"./trained_model\")\n",
    "# tokenizer.save_pretrained(\"./trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = trainer.evaluate()\n",
    "# print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"I'm feeling overwhelmed with everything going on in my life. Can you help me figure out how to deal with it? ive been trying to figure it out for the past few days.\"}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# for generating predictions\n",
    "chatbot = pipeline(\"text-generation\", model=\"./trained_model\", truncation=True, tokenizer=tokenizer)\n",
    "response = chatbot(\"I'm feeling overwhelmed with everything going on in my life. Can you help me figure out how to deal with it? \", max_length=100)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"./trained_model\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"./trained_model\")\n",
    "\n",
    "# # let's chat for 5 lines\n",
    "# for step in range(5):\n",
    "#     # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "#     new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "#     # append the new user input tokens to the chat history\n",
    "#     bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "#     # generated a response while limiting the total chat history to 1000 tokens,\n",
    "#     chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#     # pretty print last output tokens from bot\n",
    "#     print(\"EmpathAI: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human-Like-DPO-Dataset Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the previously trained model\n",
    "model_name = \"./trained_model\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt  \\\n",
      "0   Oh, I just saw the best meme - have you seen it?   \n",
      "1                   Do you have a go-to karaoke jam?   \n",
      "2  **Crafty corner** Are you good at any DIY proj...   \n",
      "3  What's your favorite type of cuisine to cook o...   \n",
      "4              Do you have a secret talent or skill?   \n",
      "\n",
      "                                              chosen  \\\n",
      "0  😂 Ah, no I haven't! I'm dying to know, what's ...   \n",
      "1  Oh, totally! 😄 I'm a sucker for a good ol' roc...   \n",
      "2  😊 I'm actually a big fan of DIY projects! I'm ...   \n",
      "3  Oh, man! I'm a total sucker for Italian food! ...   \n",
      "4  You know, I've always been fascinated by music...   \n",
      "\n",
      "                                            rejected  \n",
      "0  I'm an artificial intelligence language model,...  \n",
      "1  As a professional AI language model, I don't h...  \n",
      "2  Good day. As a continuously evolving artificia...  \n",
      "3  In accordance with my programming, I must emph...  \n",
      "4  Good day. As a professional AI language model,...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the new dataset\n",
    "human_like_dpo_ds = load_dataset(\"HumanLLMs/Human-Like-DPO-Dataset\")\n",
    "# convert the train split to a pandas DataFrame for easier manipulation\n",
    "train_df = pd.DataFrame(human_like_dpo_ds['train'])\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the cell below, after tokenizing the text, i added the `labels` field, which will be used for training the model. the `labels` should be the same as the `input_ids` shifted by one token for casual language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "[16963, 457, 25, 3966, 11, 314, 655, 2497, 262, 1266, 25336, 532, 423, 345, 1775, 340, 21747, 198, 354, 5233, 25, 30325, 224, 7900, 11, 645, 314, 4398, 470, 0, 314, 1101, 9950, 284, 760, 11, 644, 338, 262, 25336, 546, 30, 1148, 340, 257, 8258, 3797, 393, 257, 11441, 3074, 30, 1338, 359, 262, 16567, 0, 12520, 97, 96, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
      "prompt: Oh, I just saw the best meme - have you seen it?,\n",
      "chosen: 😂 Ah, no I haven't! I'm dying to know, what's the meme about? Is it a funny cat or a ridiculous situation? Spill the beans! 🤣<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# preprocess \n",
    "def preprocess_new_dataset(dataset, tokenizer):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(dataset[\"prompt\"])):\n",
    "        input_text = dataset[\"prompt\"][i]\n",
    "        target_text = dataset[\"chosen\"][i]\n",
    "\n",
    "        # format text into a tuple (prompt, chosen)\n",
    "        formatted_text = f\"prompt: {input_text},\\nchosen: {target_text}\"\n",
    "\n",
    "        # tokenize formatted text\n",
    "        tokenized_text = tokenizer(formatted_text, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "        # the labels are the same as the input_ids\n",
    "        tokenized_text[\"labels\"] = tokenized_text[\"input_ids\"].copy()\n",
    "\n",
    "        # append to lists\n",
    "        input_ids.append(tokenized_text[\"input_ids\"])\n",
    "        attention_masks.append(tokenized_text[\"attention_mask\"])\n",
    "        labels.append(tokenized_text[\"labels\"])\n",
    "\n",
    "    # return a dictionary in the form expected by Dataset.from_dict()\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "processed_conversation = preprocess_new_dataset(train_df, tokenizer)\n",
    "# debug: print first example\n",
    "print(len(processed_conversation))\n",
    "print(processed_conversation.keys())\n",
    "print(processed_conversation[\"input_ids\"][0])\n",
    "print(tokenizer.decode(processed_conversation[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after processing the dataset, it is converted back to a `Dataset` object for use in the `Trainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "human_like_convos_ds = Dataset.from_dict(processed_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./human_like_dpo_results\",             # directory to save model checkpoints\n",
    "#     num_train_epochs=3,                 # adjust based on convergence\n",
    "#     per_device_train_batch_size=4,      # increase to 16 if vram allows, or decrease to 8\n",
    "#     per_device_eval_batch_size=8, \n",
    "#     gradient_accumulation_steps=4,      # helps if batch size is small\n",
    "#     # evaluation_strategy=\"epoch\",        # evaluate at the end of each epoch\n",
    "#     save_strategy=\"epoch\",              # save model checkpoints each epoch\n",
    "#     logging_dir=\"./logs\",               # logging directory\n",
    "#     logging_steps=100,                  # adjust based on dataset size\n",
    "#     learning_rate=5e-5,                 # starndard for transformer fine-tuning\n",
    "#     warmup_steps=500,                   # helps stabilize training\n",
    "#     weight_decay=0.01,                  # regularization\n",
    "#     bf16=True,                          # enable mixed precision for speedup\n",
    "#     save_total_limit=3,                 # keep last 3 checkpoints\n",
    "#     # eval_accumulation_steps=4,          # accumuluate loss over multiple steps\n",
    "#     logging_first_step=True,            # log first step\n",
    "#     report_to=\"tensorboard\",            # enable tensorboard integration\n",
    "# )\n",
    "\n",
    "# # create trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=human_like_convos_ds,\n",
    "#     tokenizer=tokenizer,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate before fine-tuning\n",
    "# pre_eval_results = trainer.evaluate(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pre_eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.is_available())\n",
    "# print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fine-tune the model :)\n",
    "# trainer.train(resume_from_checkpoint=\"./human_like_dpo_results/checkpoint-1360\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"./human_like_dpo_model\")\n",
    "# tokenizer.save_pretrained(\"./human_like_dpo_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"I'm feeling overwhelmed with everything going on in my life. Can you help me figure out how to deal with it? ive been trying to figure it out for the past few days.\"}]\n"
     ]
    }
   ],
   "source": [
    "human_like_dpo_chatbot = pipeline(\"text-generation\", model=\"./human_like_dpo_model\", truncation=True, tokenizer=tokenizer)\n",
    "response = chatbot(\"I'm feeling overwhelmed with everything going on in my life. Can you help me figure out how to deal with it? \", max_length=100)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"../empathAI/human_like_dpo_model\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"../empathAI/human_like_dpo_model\")\n",
    "\n",
    "# # let's chat for 5 lines\n",
    "# for step in range(5):\n",
    "#     # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "#     new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "#     # append the new user input tokens to the chat history\n",
    "#     bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "#     # generated a response while limiting the total chat history to 1000 tokens,\n",
    "#     chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#     # pretty print last output tokens from bot\n",
    "#     print(\"EmpathAI: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic-persona-chat dataset fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \".//human_like_dpo_model\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8938\n",
      "8938\n",
      "8938\n"
     ]
    }
   ],
   "source": [
    "synthetic_persona_chat_ds = load_dataset(\"google/Synthetic-Persona-Chat\")\n",
    "spc_train_df = pd.DataFrame(synthetic_persona_chat_ds[\"train\"])\n",
    "spc_eval_df = pd.DataFrame(synthetic_persona_chat_ds[\"validation\"])\n",
    "print(len(spc_train_df['user 1 personas']))\n",
    "print(len(spc_train_df['user 2 personas']))\n",
    "print(len(spc_train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user 1 personas', 'user 2 personas', 'Best Generated Conversation'], dtype='object')\n",
      "User 1: Hi! I'm [user 1's name].\n",
      "User 2: Hi [user 1's name], I'm [user 2's name].\n",
      "User 1: What do you do for fun?\n",
      "User 2: I like to play video games, go to the beach, and read.\n",
      "User 1: I like to play video games too! I'm not much of a reader, though.\n",
      "User 2: What video games do you like to play?\n",
      "User 1: I like to play a lot of different games, but I'm really into competitive online games right now.\n",
      "User 2: I'm not really into competitive games, I like to play more relaxing games.\n",
      "User 1: That's cool. What kind of relaxing games do you like to play?\n",
      "User 2: I like to play puzzle games, simulation games, and story-based games.\n",
      "User 1: I've never been much of a puzzle game person, but I do like simulation games and story-based games.\n",
      "User 2: Nice! What's your favorite simulation game?\n",
      "User 1: I like Stardew Valley a lot. It's a farming game, but it's also really relaxing and fun.\n",
      "User 2: I've heard good things about that game. I might have to check it out.\n",
      "User 1: You should! It's a lot of fun.\n",
      "User 2: Well, I'm glad we met. Maybe we can play some games together sometime.\n",
      "User 1: That would be fun!\n",
      "User 2: Great! I'll send you my Steam name.\n",
      "User 1: Ok, sounds good.\n",
      "I am 32.\n",
      "I do not want a job.\n",
      "I play video games all day.\n",
      "I still live at home with my parents.\n",
      "My favorite drink is iced coffee.\n",
      "I have a black belt in karate.\n",
      "I m in a jazz band and play the saxophone.\n",
      "I vacation along lake michigan every summer.\n"
     ]
    }
   ],
   "source": [
    "print(spc_train_df.keys())\n",
    "print(spc_train_df[\"Best Generated Conversation\"][0])\n",
    "print(spc_train_df['user 1 personas'][0])\n",
    "print(spc_train_df['user 2 personas'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from scikit-learn) (2.2.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\diego\\onedrive\\desktop\\empathai\\caldito\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user1_persona': 'I am 32.\\nI do not want a job.\\nI play video games all day.\\nI still live at home with my parents.', 'user2_persona': 'My favorite drink is iced coffee.\\nI have a black belt in karate.\\nI m in a jazz band and play the saxophone.\\nI vacation along lake michigan every summer.', 'conversation_pairs': [{'prompt': \"Hi! I'm [user 1's name].\", 'response': \"Hi [user 1's name], I'm [user 2's name].\", 'prompt_persona': 'I am 32.\\nI do not want a job.\\nI play video games all day.\\nI still live at home with my parents.', 'response_persona': 'My favorite drink is iced coffee.\\nI have a black belt in karate.\\nI m in a jazz band and play the saxophone.\\nI vacation along lake michigan every summer.'}, {'prompt': 'What do you do for fun?', 'response': 'I like to play video games, go to the beach, and read.', 'prompt_persona': 'I am 32.\\nI do not want a job.\\nI play video games all day.\\nI still live at home with my parents.', 'response_persona': 'My favorite drink is iced coffee.\\nI have a black belt in karate.\\nI m in a jazz band and play the saxophone.\\nI vacation along lake michigan every summer.'}, {'prompt': \"I like to play video games too! I'm not much of a reader, though.\", 'response': 'What video games do you like to play?', 'prompt_persona': 'I am 32.\\nI do not want a job.\\nI play video games all day.\\nI still live at home with my parents.', 'response_persona': 'My favorite drink is iced coffee.\\nI have a black belt in karate.\\nI m in a jazz band and play the saxophone.\\nI vacation along lake michigan every summer.'}, {'prompt': \"I like to play a lot of different games, but I'm really into competitive online games right now.\", 'response': \"I'm not really into competitive games, I like to play more relaxing games.\", 'prompt_persona': 'I am 32.\\nI do not want a job.\\nI play video games all day.\\nI still live at home with my parents.', 'response_persona': 'My favorite drink is iced coffee.\\nI have a black belt in karate.\\nI m in a jazz band and play the saxophone.\\nI vacation along lake michigan every summer.'}, {'prompt': \"That's cool. What kind of relaxing games do you like to play?\", 'response': 'I like to play puzzle games, simulation games, and story-based games.', 'prompt_persona': 'I am 32.\\nI do not want a job.\\nI play video games all day.\\nI still live at home with my parents.', 'response_persona': 'My favorite drink is iced coffee.\\nI have a black belt in karate.\\nI m in a jazz band and play the saxophone.\\nI vacation along lake michigan every summer.'}, {'prompt': \"I've never been much of a puzzle game person, but I do like simulation games and story-based games.\", 'response': \"Nice! What's your favorite simulation game?\", 'prompt_persona': 'I am 32.\\nI do not want a job.\\nI play video games all day.\\nI still live at home with my parents.', 'response_persona': 'My favorite drink is iced coffee.\\nI have a black belt in karate.\\nI m in a jazz band and play the saxophone.\\nI vacation along lake michigan every summer.'}, {'prompt': \"I like Stardew Valley a lot. It's a farming game, but it's also really relaxing and fun.\", 'response': \"I've heard good things about that game. I might have to check it out.\", 'prompt_persona': 'I am 32.\\nI do not want a job.\\nI play video games all day.\\nI still live at home with my parents.', 'response_persona': 'My favorite drink is iced coffee.\\nI have a black belt in karate.\\nI m in a jazz band and play the saxophone.\\nI vacation along lake michigan every summer.'}, {'prompt': \"You should! It's a lot of fun.\", 'response': \"Well, I'm glad we met. Maybe we can play some games together sometime.\", 'prompt_persona': 'I am 32.\\nI do not want a job.\\nI play video games all day.\\nI still live at home with my parents.', 'response_persona': 'My favorite drink is iced coffee.\\nI have a black belt in karate.\\nI m in a jazz band and play the saxophone.\\nI vacation along lake michigan every summer.'}, {'prompt': 'That would be fun!', 'response': \"Great! I'll send you my Steam name.\", 'prompt_persona': 'I am 32.\\nI do not want a job.\\nI play video games all day.\\nI still live at home with my parents.', 'response_persona': 'My favorite drink is iced coffee.\\nI have a black belt in karate.\\nI m in a jazz band and play the saxophone.\\nI vacation along lake michigan every summer.'}]}\n"
     ]
    }
   ],
   "source": [
    "# extract multiple exchanges from conversations\n",
    "def extract_exchanges(example):\n",
    "    all_exchanges = []\n",
    "\n",
    "    user1_personas = example[\"user 1 personas\"]\n",
    "    user2_personas = example[\"user 2 personas\"]\n",
    "    bgc = example[\"Best Generated Conversation\"]\n",
    "\n",
    "    # outer loop: iterates through the whole conversation string and splits it into individual turns\n",
    "    # inner loop: iterates through the list of strings and pairs each 'user 1' line with the next 'user 2' line\n",
    "    for i in range(len(bgc)):\n",
    "        conversation_lines = bgc[i].split(\"\\n\")\n",
    "        conversation_pairs = []\n",
    "\n",
    "        # create prompt-response pairs\n",
    "        for j in range(0, len(conversation_lines)-1, 2):\n",
    "            prompt = conversation_lines[j].replace(\"User 1: \", \"\")  # remove 'user 1' prefix\n",
    "            response = conversation_lines[j+1].replace(\"User 2: \", \"\")  # remove 'user 2' prefix\n",
    "\n",
    "            # persona of both users is included with each prompt-response pair\n",
    "            conversation_pairs.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": response,\n",
    "                \"prompt_persona\": user1_personas[i],\n",
    "                \"response_persona\": user2_personas[i]\n",
    "            })\n",
    "\n",
    "        # add each converstaion pair to the main list that is to be returned\n",
    "        all_exchanges.append({\n",
    "            \"user1_persona\": user1_personas[i],\n",
    "            \"user2_persona\": user2_personas[i],\n",
    "            \"conversation_pairs\": conversation_pairs\n",
    "        })\n",
    "\n",
    "    return all_exchanges\n",
    "        \n",
    "all_exchanges = extract_exchanges(spc_train_df)\n",
    "print(all_exchanges[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for processing the conversations into a flat list of examples\n",
    "def prepare_data_for_trainer(all_exchanges):\n",
    "    examples = []\n",
    "\n",
    "    for exchange in all_exchanges:\n",
    "        for pair in exchange[\"conversation_pairs\"]:\n",
    "            if \"prompt_persona\" in pair and \"response_persona\" in pair:\n",
    "                prompt_with_persona = f\"Persona: {pair[\"prompt_persona\"]}\\nMessage: {pair[\"prompt\"]}\"\n",
    "            else:\n",
    "                prompt_with_persona = pair[\"prompt\"]\n",
    "\n",
    "            examples.append({\n",
    "                \"prompt\": prompt_with_persona,\n",
    "                \"response\": pair[\"response\"]\n",
    "            })\n",
    "\n",
    "    return examples\n",
    "\n",
    "# process the data\n",
    "all_examples = prepare_data_for_trainer(all_exchanges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_examples, temp_examples = train_test_split(all_examples, test_size=0.2, random_state=42)\n",
    "val_examples, test_examples = train_test_split(temp_examples, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to Hugging Face Datasets\n",
    "spc_train_dataset = Dataset.from_list(train_examples)\n",
    "spc_val_dataset = Dataset.from_list(val_examples)\n",
    "spc_test_dataset = Dataset.from_list(test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100117/100117 [00:09<00:00, 11117.32 examples/s]\n",
      "Map: 100%|██████████| 12515/12515 [00:01<00:00, 12083.70 examples/s]\n",
      "Map: 100%|██████████| 12515/12515 [00:01<00:00, 12154.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenization function\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # for autoregressive models like DialoGPT\n",
    "    inputs = [f\"{prompt} <|response|> {response} <|endoftext|>\" \n",
    "              for prompt, response in zip(examples[\"prompt\"], examples[\"response\"])]\n",
    "    \n",
    "    result = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=256)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "# apply tokenization\n",
    "tokenized_train = spc_train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val = spc_val_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = spc_test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diego\\AppData\\Local\\Temp\\ipykernel_14384\\1116729622.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./spc_results\",         # directory to save model checkpoints\n",
    "    num_train_epochs=3,                 # adjust based on convergence\n",
    "    per_device_train_batch_size=8,      # increase to 16 if vram allows, or decrease to 8\n",
    "    per_device_eval_batch_size=8, \n",
    "    gradient_accumulation_steps=2,      # helps if batch size is small\n",
    "    evaluation_strategy=\"epoch\",        # evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",              # save model checkpoints each epoch\n",
    "    logging_dir=\"./logs/spc_logs\",               # logging directory\n",
    "    logging_steps=100,                  # adjust based on dataset size\n",
    "    learning_rate=5e-5,                 # starndard for transformer fine-tuning\n",
    "    warmup_steps=500,                   # helps stabilize training\n",
    "    weight_decay=0.01,                  # regularization\n",
    "    bf16=True,                          # enable mixed precision for speedup\n",
    "    save_total_limit=3,                 # keep last 3 checkpoints\n",
    "    eval_accumulation_steps=4,          # accumuluate loss over multiple steps\n",
    "    logging_first_step=True,            # log first step\n",
    ")\n",
    "\n",
    "# create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diego\\AppData\\Local\\Temp\\ipykernel_14384\\967146244.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='718' max='718' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [718/718 00:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate before fine-tuning\n",
    "pre_eval_results = trainer.evaluate(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24718457460403442, 'eval_model_preparation_time': 0.0094, 'eval_runtime': 54.4664, 'eval_samples_per_second': 105.331, 'eval_steps_per_second': 13.182}\n"
     ]
    }
   ],
   "source": [
    "print(pre_eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18771' max='18771' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18771/18771 1:13:08, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.171200</td>\n",
       "      <td>0.159699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.140500</td>\n",
       "      <td>0.145910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18771, training_loss=0.1693690795175345, metrics={'train_runtime': 4388.447, 'train_samples_per_second': 68.441, 'train_steps_per_second': 4.277, 'total_flos': 1.394509101882409e+17, 'train_loss': 0.1693690795175345, 'epoch': 2.99960047942469})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model :)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./spc_model\\\\tokenizer_config.json',\n",
       " './spc_model\\\\special_tokens_map.json',\n",
       " './spc_model\\\\vocab.json',\n",
       " './spc_model\\\\merges.txt',\n",
       " './spc_model\\\\added_tokens.json',\n",
       " './spc_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./spc_model\")\n",
    "tokenizer.save_pretrained(\"./spc_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caldito",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
